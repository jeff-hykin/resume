goal:
    What problems have been solved in relation to the problem of affordance based perception?
    
dud searches:
    - search: ecological affect based perception machine learning
      results: conflated with human emotion recognition and ecological as in environmental/urbanism
    - search: affect based perception machine learning
    - search: agent driven affect based perception machine learning
    - search: agent driven affect based perception machine learning -emotion
    - search: agent driven "affect" based perception machine learning -emotion
    - search: ecologically-general representation machine learning
      results: closer, but pretty much just the mouse study I already had
    - search: ecological representation machine learning
      results: just machine learning for ecology stuff
    - search: utility based representation learning AI
    
    
    

sources:
    "AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection":
        relevent: true
        year: 2018
        url: https://ieeexplore.ieee.org/abstract/document/8460902
        search:
            google_scholar: affordance based object recognition machine learning
    
    Immense World:
        relevant: true
        media_type: book
        notes: |
            The outcomes are really impressive (bat hearing, dog smelling) so much is possible with simple logic on top of these worlds of perceptions. But I can't help but think robotic sensors pale in comparison -- so badly as to make mimicking most of the behavior impossible.
        found_through:
            person: Dr. Murphy

    The Ecological Approach for visual perception:
        relevant: true
        media_type: book
        author: JJ Gibson
        notes: |
            Harry Heft did a pretty good job of summarizing the ideas in this paper.
            Lots of the ideas are philosophical with a bit of a gap on how to implement them.

    Bioinspired Tactile Sensation Based on Synergistic Microcrack-Bristle Structure Design toward High Mechanical Sensitivity and Direction-Resolving Capability:
        relevant: false
        media_type: article
        url: https://spj.science.org/doi/10.34133/research.0172
        notes: |
            Interesting, but wouldn't be practical for me to replicate or use for anything in practice.

    The Ecological Approach for visual perception video:
        relevant: true
        media_type: video
        author: Harry Heft
        url: https://www.youtube.com/watch?v=k4fKBqu-Ris
        found_through:
            person: Dr. Murphy
        notes: |
            1. Things in the environment are a modifier function on the transformation of your motion through the world. Ex: a disco ball changes your visual feedback as you walk. If both stand still, then there's no modification. When our estimation of that modifier function converges and becomes reliable, then we are fully perceiving that thing. E.g. when we know what the back of a couch looks like by seeing the front, we are perceiving the couch. When we are surprised, then we didn't fully perceive it. This “am I having a stroke image is all-sensing no-perceiving
            (see image in doc: https://docs.google.com/document/d/1iIInU8a6Vtvp77xJAIvQpRbHw36JBvdOKkETXYm7bno/edit?tab=t.0 )
            2. Objects might be in the same category (tree) but for us they fall in many functional categories, such as “can I easily climb it” where things like “how annoying is the sap” can play a role. The climb-ability is in the tree, but is relative to the perceiver.
    
    "Ecological approaches to perceptual learning: learning to perceive and perceiving as learning":
        year: 2019
        relevant: false
        url: https://journals.sagepub.com/doi/full/10.1177/1059712319854687
        notes: |
            cites JJ Gibson
            could be a good reference to modern work on ecological perception in psychology
        found_through:
            search:
                google_scholar: ecological affect based perception machine learning
    
    Measuring and modeling the motor system with machine learning:
        relevant: false
        media_type: article
        found_through:
            search: https://www.sciencedirect.com/search?references=Gary%20Klein&qs=robotics&years=2018%2C2019%2C2020%2C2021%2C2022%2C2023%2C2024%2C2025&show=100&subjectAreas=2200%2C1700%2C2800%2C3200
        
        summary: |
            S2 TL;DR: This review discusses the growing use of machine learning: from pose estimation, kinematic analyses, dimensionality reduction, and closed-loop feedback, to its use in understanding neural correlates and untangling sensorimotor systems.
            
    Contrasting action and posture coding with hierarchical deep neural network models of proprioception:
        year: 2020
        relevant: true
        media_type: article
        url: https://www.semanticscholar.org/paper/3bdff5d56330b218dba04e1618f1410d52edb1e5
        found_through:
            cites: Measuring and modeling the motor system with machine learning

        notes: |
            probably the most relevant of the work related to the above source.
            Does directly tie motors with perception.

    How well do deep neural networks trained on object recognition characterize the mouse visual system:
        year: 2019
        relevant: true
        media_type: article
        url: https://www.semanticscholar.org/paper/4b45f999e6f3cde0c63fc3e4a4eef589497f1ebb
        found_through:
            prior_work: Contrasting action and posture coding with hierarchical deep neural network models of proprioception

        notes: |
            probably most relevant work so far
    
    Mouse visual cortex as a limited resource system that self-learns an ecologically-general representation:
        year: 2021
        relevant: true
        super_relvelevant: true
        media_type: article
        url: https://www.semanticscholar.org/paper/d340e4fa9cf36e5e363d2c22d354ef21bf9ab22a
        found_through:
            prior_work: Contrasting action and posture coding with hierarchical deep neural network models of proprioception
        read_more_than_the_abtract: true
        fully_annotated_pdf: true
        notes: |
            "Contrastive AlexNet" best model of mouse visual perception so far
            
            "neural predictivity from an ecological point of view"
            
            uses a "visual rodent" simulator as part of validation and testing of affect based perception (e.g. naviation but not manipulation)
            
            claims contrastive objectives are input-domain agnostic, which doesn't make sense to me due to the agumentation needed for contrastive learning. I'm still not sure why they say "constrastive objective" instead of "constrastive loss".
            
            Focues on the mouse visual cortex rather than the general idea of affective perception
            does say "providing a high-fidelity quantitative model of mouse visual cortex and identifying key structural and functional principles underlying that model's success."
            
            "self-supervised, contrastive algorithms provide the best correspondence to mouse visual responses" (in comparison to supervised)
        additional_line_of_research: 
            how does contrastive learning work?: |
                issue: it needs a data augmentation strategy, which is basically domain specific knowledge. Possibly there is a way to fix this, but after searching for "contrastive learning with no augmentation" I couldn't find any general solutions.
            
    Multi-scale hierarchical neural network models that bridge from single neurons in the primate primary visual cortex to object recognition behavior:
        year: 2021
        media_type: article
        relevant: true # probably/maybe
        url: https://www.semanticscholar.org/paper/83c41107206590a2e0fe5fe415e7f33ea1d55ea4
        found_through:
            cited_by: Mouse visual cortex as a limited resource system that self-learns an ecologically-general representation
        notes: |
            no mention of affects or ecological justification
        summary: |
            Primate visual object recognition relies on the representations in cortical areas at the top of the ventral stream that are computed by a complex, hierarchical network of neural populations. While recent work has created reasonably accurate image-computable hierarchical neural network models of those neural stages, those models do not yet bridge between the properties of individual neurons and the overall emergent behavior of the ventral stream. One reason we cannot yet do this is that individual artificial neurons in multi-stage models have not been shown to be functionally similar to individual biological neurons. Here, we took an important first step by building and evaluating hundreds of hierarchical neural network models in how well their artificial single neurons approximate macaque primary visual cortical (V1) neurons. We found that single neurons in certain models are surprisingly similar to their biological counterparts and that the distributions of single neuron properties, such as those related to orientation and spatial frequency tuning, approximately match those in macaque V1. Critically, we observed that hierarchical models with V1 stages that better match macaque V1 at the single neuron level are also more aligned with human object recognition behavior. Finally, we show that an optimized classical neuroscientific model of V1 is more functionally similar to primate V1 than all of the tested multi-stage models, suggesting room for further model improvements with tangible payoffs in closer alignment to human behavior. These results provide the first multi-stage, multi-scale models that allow our field to ask precisely how the specific properties of individual V1 neurons relate to recognition behavior. Highlights Image-computable hierarchical neural network models can be naturally extended to create hierarchical “brain models” that allow direct comparison with biological neural networks at multiple scales – from single neurons, to population of neurons, to behavior. Single neurons in some of these hierarchical brain models are functionally similar to single neurons in macaque primate visual cortex (V1) Some hierarchical brain models have processing stages in which the entire distribution of artificial neuron properties closely matches the biological distributions of those same properties in macaque V1 Hierarchical brain models whose V1 processing stages better match the macaque V1 stage also tend to be more aligned with human object recognition behavior at their output stage
        
        
            
    A Unifying Principle for the Functional Organization of Visual Cortex:
        year: 2023
        relevant: false
        interesting: true
        media_type: article
        url: https://www.semanticscholar.org/paper/4820597fb52de7d849c655b8ee5e61ccb3b1f81c
    
    Deep Spiking Neural Networks with High Representation Similarity Model Visual Pathways of Macaque and Mouse:
        year: 2023
        relevant: false
        interesting: true
        media_type: article
        url: https://www.semanticscholar.org/paper/462b999f9e47915c89a0c70d797d3e82276f8410
        found_through:
            cites: Mouse visual cortex as a limited resource system that self-learns an ecologically-general representation
    
    Feasibility Theory Reconciles and Informs Alternative Approaches to Neuromuscular Control:
        year: 2018
        relevant: false
        notes: |  
            could lead to some relevant work
            
    
    The Neural Basis of Haptic Perception:
        year: 2018
        media_type: chapter
        book: Stevens' Handbook of Experimental Psychology and Cognitive Neuroscience
        relevant: false
        notes: |
            could be a good reference on touch perception
        found_through:
            cites: Mouse visual cortex as a limited resource system that self-learns an ecologically-general representation
    
    "Deep Learning for Human Affect Recognition: Insights and New Developments":
        year: 2019
        relevant: false
        notes: |
            means affect in a different way, e.g. detecting emotions of humans
        found_through:
            search:
                google_scholar: affect based perception machine learning
                url: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C44&q=affect+based+perception+machine+learning&btnG=&oq=affect+based+perception+machine+lae
    
    How Well Do Unsupervised Learning Algorithms Model Human Real-time and Life-long Learning?:
        year: 2022
        relevant: false
        interesting: true
        media_type: article
        url: https://www.semanticscholar.org/paper/4dac091ce96bb8c945c20cf3c5902bea40e6cee4
        found_through:
            cites: Mouse visual cortex as a limited resource system that self-learns an ecologically-general representation
        
        summary: |
            Humans learn from visual inputs at multiple timescales, both rapidly and flexibly acquiring visual knowledge over short periods, and robustly accumulating online learning progress over longer periods. Modeling these powerful learning capabilities is an important problem for computational visual cognitive science, and models that could replicate them would be of substantial utility in real-world computer vision settings. In this work, we establish benchmarks for both real-time and life-long continual visual learning. Our real-time learning benchmark measures a model's ability to match the rapid visual behavior changes of real humans over the course of minutes and hours, given a stream of visual inputs. Our life-long learning benchmark evaluates the performance of models in a purely online learning curriculum obtained directly from child visual experience over the course of years of development. We evaluate a spectrum of recent deep self-supervised visual learning algorithms on both benchmarks, finding that none of them perfectly match human performance, though some algorithms perform substantially better than others. Interestingly, algorithms embodying recent trends in self-supervised learning - including BYOL, SwAV and MAE - are substantially worse on our benchmarks than an earlier generation of self-supervised algorithms such as SimCLR and MoCo-v2. We present analysis indicating that the failure of these newer algorithms is primarily due to their inability to handle the kind of sparse low-diversity datastreams that naturally arise in the real world, and that actively leveraging memory through negative sampling - a mechanism eschewed by these newer algorithms - appears useful for facilitating learning in such low-diversity environments. We also illustrate a complementarity between the short and long timescales in the two benchmarks, showing how requiring a single learning algorithm to be locally context-sensitive enough to match real-time learning changes while stable enough to avoid catastrophic forgetting over the long term induces a trade-off that human-like algorithms may have to straddle. Taken together, our benchmarks establish a quantitative way to directly compare learning between neural networks models and human learners, show how choices in the mechanism by which such algorithms handle sample comparison and memory strongly impact their ability to match human learning abilities, and expose an open problem space for identifying more flexible and robust visual self-supervision algorithms.
        
    Task-driven neural network models predict neural dynamics of proprioception:
        relevant: false
        media_type: article
        url: https://www.semanticscholar.org/paper/5e3a1ab688007ce33fb4b6075a7c223d7e5edc6e
        found_through:
            cites: Measuring and modeling the motor system with machine learning

        notes: |
            heavy relation to neurology, but not necessarily related to affect based recognition.
    The neuroconnectionist research programme:
        year: 2023
        url: https://www.nature.com/articles/s41583-023-00705-w
        notes: |
            could be good background reading for the neuology + ML stuff
        summary: |
            Artificial neural networks (ANNs) inspired by biology are beginning to be widely used to model behavioural and neural data, an approach we call ‘neuroconnectionism’. ANNs have been not only lauded as the current best models of information processing in the brain but also criticized for failing to account for basic cognitive functions. In this Perspective article, we propose that arguing about the successes and failures of a restricted set of current ANNs is the wrong approach to assess the promise of neuroconnectionism for brain science. Instead, we take inspiration from the philosophy of science, and in particular from Lakatos, who showed that the core of a scientific research programme is often not directly falsifiable but should be assessed by its capacity to generate novel insights. Following this view, we present neuroconnectionism as a general research programme centred around ANNs as a computational language for expressing falsifiable theories about brain computation. We describe the core of the programme, the underlying computational framework and its tools for testing specific neuroscientific hypotheses and deriving novel understanding. Taking a longitudinal view, we review past and present neuroconnectionist projects and their responses to challenges and argue that the research programme is highly progressive, generating new and otherwise unreachable insights into the workings of the brain.
        
        relevant: false
        
    'SUBTLE: An unsupervised platform with temporal link embedding that maps animal behavior':
        relevant: false
        media_type: article
        url: https://www.semanticscholar.org/paper/df3e41cd31e3b433ccfbf009383fe027061bfe56
        found_through:
            cites: Measuring and modeling the motor system with machine learning

        notes: |
            looks at animals but not necessarily through an affective lens 
    
    Computer Vision, Machine Learning, and the Promise of Phenomics in Ecology and Evolutionary Biology:
        relevant: false
        media_type: article
        url: https://www.frontiersin.org/journals/ecology-and-evolution/articles/10.3389/fevo.2021.642774/full
        notes: |
            using ML for normal ecology stuff but not affective perception
        found_through:
            search:
                google_scholar: agent driven "affect" based perception machine learning -emotion
    
    Multi-Agent Active Perception Based on Reinforcement Learning and POMDP:
        relevant: false
        media_type: article
        url: https://ieeexplore.ieee.org/abstract/document/10486889
        notes: |
            pretty much just some random RL paper
        found_through:
            search:
                google_scholar: agent driven "affect" based perception machine learning -emotion
    
    How perceptions of intelligence and anthropomorphism affect adoption of personal intelligent agents:
        relevant: false
        media_type: article
        url: https://link.springer.com/article/10.1007/s12525-020-00411-w
        notes: |
            might be useful for survivor buddy
